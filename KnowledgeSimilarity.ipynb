{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf8bbd0-887e-4df7-a9f7-3ef3ba5a688a",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "The research paper uses sentence embedding on noun and noun phrases. This analysis is to see of other modern approaches can reach same or better score. Ultimately, we want to see if the new approach at least mataches the high scores annotated. \n",
    "\n",
    "1. Compare embedding of entire resume to individual entities of a category. Expect sim,ilarity to be less than the research paper\n",
    "2. Colbert index and search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c245f36-49ba-4ef3-b96f-1e681f581b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676cf421-d851-47dd-8ec3-e49f7b6085c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  onetsoc_code         job_title               knowledge_entity scale_id  \\\n",
      "0   11-1011.00  Chief Executives  Administration and Management       IM   \n",
      "1   11-1011.00  Chief Executives  Administration and Management       LV   \n",
      "2   11-1011.00  Chief Executives                 Administrative       IM   \n",
      "3   11-1011.00  Chief Executives                 Administrative       LV   \n",
      "4   11-1011.00  Chief Executives       Economics and Accounting       IM   \n",
      "\n",
      "   data_value  \n",
      "0        4.78  \n",
      "1        6.50  \n",
      "2        2.42  \n",
      "3        2.69  \n",
      "4        4.04  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the O*NET Knowledge Excel file\n",
    "knowledge_file = \"data/annotations_scenario_1/Knowledge.xlsx\"  # Update with the actual filename\n",
    "df_onet = pd.read_excel(knowledge_file)\n",
    "\n",
    "# Select relevant columns\n",
    "df_onet = df_onet[[\"O*NET-SOC Code\", \"Title\", \"Element Name\", \"Scale ID\", \"Data Value\"]]\n",
    "\n",
    "# Filter for only importance (IM) and level (LV)\n",
    "df_onet = df_onet[df_onet[\"Scale ID\"].isin([\"IM\", \"LV\"])]\n",
    "\n",
    "# Rename columns for consistency\n",
    "df_onet.rename(columns={\n",
    "    \"O*NET-SOC Code\": \"onetsoc_code\",\n",
    "    \"Title\": \"job_title\",\n",
    "    \"Element Name\": \"knowledge_entity\",\n",
    "    \"Scale ID\": \"scale_id\",\n",
    "    \"Data Value\": \"data_value\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the processed data in a Pandas DataFrame\n",
    "print(df_onet.head())  # Show the first few rows\n",
    "\n",
    "# Save to CSV if you want to inspect it further\n",
    "df_onet.to_csv(\"data/annotations_scenario_1/processed_onet_knowledge.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b29abcd-4c17-4719-99d0-637e3fc1cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  onetsoc_code                            job_title  \\\n",
      "0   11-1011.00                     Chief Executives   \n",
      "1   11-1011.03        Chief Sustainability Officers   \n",
      "2   11-1021.00      General and Operations Managers   \n",
      "3   11-1031.00                          Legislators   \n",
      "4   11-2011.00  Advertising and Promotions Managers   \n",
      "\n",
      "                                     job_description  \n",
      "0  Determine and formulate policies and provide o...  \n",
      "1  Communicate and coordinate with management, sh...  \n",
      "2  Plan, direct, or coordinate the operations of ...  \n",
      "3  Develop, introduce, or enact laws and statutes...  \n",
      "4  Plan, direct, or coordinate advertising polici...  \n"
     ]
    }
   ],
   "source": [
    "# Load the O*NET Knowledge Excel file\n",
    "occupation_file = \"data/annotations_scenario_1/Occupation Data.xlsx\"  # Update with the actual filename\n",
    "\n",
    "df_occupation = pd.read_excel(occupation_file)\n",
    "\n",
    "# Select relevant columns\n",
    "df_occupation = df_occupation[[\"O*NET-SOC Code\", \"Title\", \"Description\"]]\n",
    "\n",
    "# Rename columns for consistency\n",
    "df_occupation.rename(columns={\n",
    "    \"O*NET-SOC Code\": \"onetsoc_code\",\n",
    "    \"Title\": \"job_title\",\n",
    "    \"Description\": \"job_description\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_occupation.head())\n",
    "\n",
    "# Save to CSV for further inspection (optional)\n",
    "df_occupation.to_csv(\"data/annotations_scenario_1/processed_onet_occupation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541b9ae5-4db9-45fa-8548-e067cea3e2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   resume_id             job_title               knowledge_entity  \\\n",
      "0          4  Computer Programmers  Administration and Management   \n",
      "2          4  Computer Programmers                 Administrative   \n",
      "4          4  Computer Programmers       Economics and Accounting   \n",
      "6          4  Computer Programmers            Sales and Marketing   \n",
      "8          4  Computer Programmers  Customer and Personal Service   \n",
      "\n",
      "   similarity_score  \n",
      "0          0.309047  \n",
      "2          0.231593  \n",
      "4          0.223551  \n",
      "6          0.269907  \n",
      "8          0.232240  \n",
      "âœ… Similarity matrix saved as 'resume_knowledge_similarity_matrix.csv'.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(\"data/annotations_scenario_1/annotations_scenario_1.db\")\n",
    "\n",
    "# Step 1: Query 10 resumes with rating = 5\n",
    "query_resumes = \"\"\"\n",
    "SELECT r.id AS resume_id, r.resume_text, pj.job_title\n",
    "FROM resumes r\n",
    "JOIN annotations a ON r.id = a.resume_id\n",
    "JOIN predicted_jobs pj ON r.id = pj.resume_id\n",
    "WHERE a.rating = 5\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "df_resumes = pd.read_sql_query(query_resumes, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Step 2: Load the O*NET knowledge dataset (previously processed)\n",
    "df_onet = pd.read_csv(\"data/annotations_scenario_1/processed_onet_knowledge.csv\")\n",
    "\n",
    "# Step 3: Initialize an empty list to store similarity results\n",
    "similarity_results = []\n",
    "\n",
    "# Step 4: Compute similarity for each resume and its corresponding job knowledge entities\n",
    "for _, row in df_resumes.iterrows():\n",
    "    resume_id = row[\"resume_id\"]\n",
    "    resume_text = row[\"resume_text\"]\n",
    "    job_title = row[\"job_title\"]\n",
    "\n",
    "    # Get knowledge entities for this job title\n",
    "    df_knowledge = df_onet[df_onet[\"job_title\"] == job_title]\n",
    "\n",
    "    if df_knowledge.empty:\n",
    "        print(f\"âš ï¸ No knowledge entities found for job: {job_title} (Resume ID: {resume_id})\")\n",
    "        continue  # Skip if no knowledge data exists for this job\n",
    "\n",
    "    # Generate embeddings\n",
    "    resume_embedding = model.encode(resume_text, convert_to_numpy=True)\n",
    "    knowledge_embeddings = df_knowledge[\"knowledge_entity\"].apply(lambda x: model.encode(x, convert_to_numpy=True))\n",
    "\n",
    "    # Compute similarity\n",
    "    similarity_scores = cosine_similarity([resume_embedding], list(knowledge_embeddings))\n",
    "\n",
    "    # Store results\n",
    "    for knowledge_entity, score in zip(df_knowledge[\"knowledge_entity\"], similarity_scores[0]):\n",
    "        similarity_results.append({\"resume_id\": resume_id, \"job_title\": job_title, \"knowledge_entity\": knowledge_entity, \"similarity_score\": score})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_similarity = pd.DataFrame(similarity_results)\n",
    "\n",
    "# Remove duplicates if any remain\n",
    "df_similarity.drop_duplicates(inplace=True)\n",
    "\n",
    "# Print a preview of the similarity matrix\n",
    "print(df_similarity.head())\n",
    "\n",
    "# Save to CSV for further analysis\n",
    "df_similarity.to_csv(\"data/annotations_scenario_1/resume_knowledge_similarity_matrix.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Similarity matrix saved as 'resume_knowledge_similarity_matrix.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c1a2a76-0037-49f3-aeb9-9a039b95b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Highest Similarity Score:\n",
      "     resume_id                                     job_title  \\\n",
      "346          5                   Computer Network Architects   \n",
      "412          5                          Computer Programmers   \n",
      "478          5         Computer Systems Engineers/Architects   \n",
      "544          5  Computer and Information Research Scientists   \n",
      "610          5                            Robotics Engineers   \n",
      "\n",
      "              knowledge_entity  similarity_score  \n",
      "346  Computers and Electronics          0.418219  \n",
      "412  Computers and Electronics          0.418219  \n",
      "478  Computers and Electronics          0.418219  \n",
      "544  Computers and Electronics          0.418219  \n",
      "610  Computers and Electronics          0.418219  \n"
     ]
    }
   ],
   "source": [
    "# Print the highest similarity score\n",
    "max_similarity = df_similarity[\"similarity_score\"].max()\n",
    "highest_match = df_similarity[df_similarity[\"similarity_score\"] == max_similarity]\n",
    "\n",
    "print(\"\\nðŸŽ¯ Highest Similarity Score:\")\n",
    "print(highest_match)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8b0a51-9668-4f0b-a8ba-15b0bf776852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "# Load ColBERT-based RAG model from Ragatouille\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919b5fe-b5e4-447e-ad37-b394b2222d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Resume ID: 4...\n",
      "New index_name received! Updating current index_name (resume_4) to resume_4\n",
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Mar 28, 14:12:15] #> Note: Output directory .ragatouille/colbert/indexes/resume_4 already exists\n",
      "\n",
      "\n",
      "[Mar 28, 14:12:15] #> Will delete 10 files already at .ragatouille/colbert/indexes/resume_4 in 20 seconds...\n",
      "#> Starting...\n",
      "#> Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nranks = 2 \t num_gpus = 2 \t device=1\n",
      "[Mar 28, 14:12:40] [1] \t\t #> Encoding 2 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nranks = 2 \t num_gpus = 2 \t device=0\n",
      "[Mar 28, 14:12:43] [0] \t\t #> Encoding 4 passages..\n",
      "[Mar 28, 14:12:44] [1] \t\t avg_doclen_est = 139.625 \t len(local_sample) = 2\n",
      "[Mar 28, 14:12:44] [0] \t\t avg_doclen_est = 139.625 \t len(local_sample) = 4\n",
      "[Mar 28, 14:12:44] [0] \t\t Creating 256 partitions.\n",
      "[Mar 28, 14:12:44] [0] \t\t *Estimated* 837 embeddings.\n",
      "[Mar 28, 14:12:44] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/resume_4/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sub_sample = torch.load(sub_sample_path)\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/infra/launcher.py\", line 134, in setup_new_process\n",
      "    return_val = callee(config, *args)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py\", line 33, in encode\n",
      "    encoder.run(shared_lists)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py\", line 68, in run\n",
      "    self.train(shared_lists) # Trains centroids from selected passages\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py\", line 232, in train\n",
      "    centroids = self._train_kmeans(sample, shared_lists)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py\", line 304, in _train_kmeans\n",
      "    centroids = compute_faiss_kmeans(*args_)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py\", line 502, in compute_faiss_kmeans\n",
      "    kmeans = faiss.Kmeans(dim, num_partitions, niter=kmeans_niters, gpu=use_gpu, verbose=True, seed=123)\n",
      "             ^^^^^^^^^^^^\n",
      "AttributeError: module 'faiss' has no attribute 'Kmeans'\n",
      "[rank0]:[W328 14:12:44.411373941 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "[rank1]:[E328 14:22:44.160111236 ProcessGroupNCCL.cpp:616] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.\n",
      "[rank1]:[E328 14:22:44.186477360 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 1] Exception (either an error or timeout) detected by watchdog at work: 5, last enqueued NCCL work: 5, last completed NCCL work: 4.\n",
      "[rank1]:[E328 14:22:44.186518808 ProcessGroupNCCL.cpp:1834] [PG ID 0 PG GUID 0(default_pg) Rank 1] Timeout at NCCL work: 5, last enqueued NCCL work: 5, last completed NCCL work: 4.\n",
      "[rank1]:[E328 14:22:44.186526252 ProcessGroupNCCL.cpp:630] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "[rank1]:[E328 14:22:44.186531281 ProcessGroupNCCL.cpp:636] [Rank 1] To avoid data inconsistency, we are taking the entire process down.\n",
      "[rank1]:[E328 14:22:44.200038549 ProcessGroupNCCL.cpp:1595] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.\n",
      "Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x787dc539f446 in /opt/conda/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x787d7ac19772 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x787d7ac20bb3 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x787d7ac2261d in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: <unknown function> + 0x145c0 (0x787dc55065c0 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch.so)\n",
      "frame #5: <unknown function> + 0x9ca94 (0x787dc6207a94 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #6: __clone + 0x44 (0x787dc6294a34 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "terminate called after throwing an instance of 'c10::DistBackendError'\n",
      "  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.\n",
      "Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x787dc539f446 in /opt/conda/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x787d7ac19772 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x787d7ac20bb3 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x787d7ac2261d in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: <unknown function> + 0x145c0 (0x787dc55065c0 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch.so)\n",
      "frame #5: <unknown function> + 0x9ca94 (0x787dc6207a94 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #6: __clone + 0x44 (0x787dc6294a34 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x787dc539f446 in /opt/conda/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0xe4271b (0x787d7a88f71b in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: <unknown function> + 0x145c0 (0x787dc55065c0 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch.so)\n",
      "frame #3: <unknown function> + 0x9ca94 (0x787dc6207a94 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #4: __clone + 0x44 (0x787dc6294a34 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Group by resume_id to ensure one resume per ID\n",
    "df_resumes_grouped = df_resumes.groupby(\"resume_id\")[\"resume_text\"].first().reset_index()\n",
    "\n",
    "# Step 2: Index Each Resume Independently\n",
    "for resume_id, resume_text in zip(df_resumes_grouped[\"resume_id\"], df_resumes_grouped[\"resume_text\"]):\n",
    "    index_name = f\"resume_{resume_id}\"  # Unique index name per resume\n",
    "    \n",
    "    print(f\"Indexing Resume ID: {resume_id}...\")  # Debugging output\n",
    "    RAG.index(\n",
    "        collection=[resume_text],  # Store the full resume as a single document\n",
    "        index_name=index_name,\n",
    "        max_document_length=180,\n",
    "        split_documents=True  # Ragatouille will handle chunking\n",
    "    )\n",
    "\n",
    "print(\"âœ… All resumes have been indexed successfully (only once per ID)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26addeee-826b-4de9-bae5-3d8a7378e058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching Resume ID: 4...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ” Searching Resume ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging output\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Perform search using all knowledge entities as queries\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m retrieved_docs \u001b[38;5;241m=\u001b[39m \u001b[43mRAG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknowledge_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Parse JSON response and store results\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m retrieved_docs:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ragatouille/RAGPretrainedModel.py:315\u001b[0m, in \u001b[0;36mRAGPretrainedModel.search\u001b[0;34m(self, query, index_name, k, force_fast, zero_index_ranks, doc_ids, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    285\u001b[0m     query: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    292\u001b[0m ):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Query an index.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_fast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_index_ranks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_index_ranks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdoc_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ragatouille/models/colbert.py:392\u001b[0m, in \u001b[0;36mColBERT.search\u001b[0;34m(self, query, index_name, k, force_fast, zero_index_ranks, doc_ids)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# TODO We may want to load an existing index here instead;\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m#      For now require that either index() was called, or an existing one was loaded.\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    394\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m     force_fast\u001b[38;5;241m=\u001b[39mforce_fast,\n\u001b[1;32m    405\u001b[0m )\n\u001b[1;32m    407\u001b[0m to_return \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Prepare queries (all O*NET knowledge entities)\n",
    "knowledge_queries = df_onet[\"knowledge_entity\"].drop_duplicates().tolist() # All queries at once\n",
    "\n",
    "# Step 2: Search for each indexed resume\n",
    "similarity_results = []\n",
    "for resume_id in df_resumes[\"resume_id\"].unique():\n",
    "    index_name = f\"resume_{resume_id}\"  # Resume index name\n",
    "\n",
    "    print(f\"ðŸ” Searching Resume ID: {resume_id}...\")  # Debugging output\n",
    "\n",
    "    # Perform search using all knowledge entities as queries\n",
    "    retrieved_docs = RAG.search(query=knowledge_queries, index_name=index_name, k=3)  \n",
    "\n",
    "    # Parse JSON response and store results\n",
    "    for doc in retrieved_docs:\n",
    "        knowledge_entity = doc[\"query\"]  # The knowledge entity used for retrieval\n",
    "        matched_text = doc[\"text\"]  # The matching chunk from the resume\n",
    "        similarity_score = doc[\"score\"]  # Similarity score\n",
    "\n",
    "        similarity_results.append({\n",
    "            \"resume_id\": resume_id,\n",
    "            \"knowledge_entity\": knowledge_entity,\n",
    "            \"matched_resume_chunk\": matched_text,\n",
    "            \"similarity_score\": round(similarity_score, 4)\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_similarity = pd.DataFrame(similarity_results)\n",
    "\n",
    "# Step 3: Find the highest similarity score\n",
    "max_similarity = df_similarity[\"similarity_score\"].max()\n",
    "highest_match = df_similarity[df_similarity[\"similarity_score\"] == max_similarity]\n",
    "\n",
    "print(\"\\nðŸŽ¯ Highest Similarity Score using Ragatouille (Knowledge as Queries):\")\n",
    "print(highest_match)\n",
    "\n",
    "# Save to CSV\n",
    "df_similarity.to_csv(\"data/annotations_scenario_1/ragatouille_resume_knowledge_similarity_matrix.csv\", index=False)\n",
    "print(\"âœ… Similarity matrix saved as 'ragatouille_resume_knowledge_similarity_matrix.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac578a2b-35c1-469a-a2a7-b650bee15425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New index_name received! Updating current index_name (resume_4) to resume_4\n",
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Mar 27, 20:31:43] #> Note: Output directory .ragatouille/colbert/indexes/resume_4 already exists\n",
      "\n",
      "\n",
      "[Mar 27, 20:31:43] #> Will delete 10 files already at .ragatouille/colbert/indexes/resume_4 in 20 seconds...\n",
      "[Mar 27, 20:32:04] [0] \t\t #> Encoding 6 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 27, 20:32:05] [0] \t\t avg_doclen_est = 136.0 \t len(local_sample) = 6\n",
      "[Mar 27, 20:32:05] [0] \t\t Creating 256 partitions.\n",
      "[Mar 27, 20:32:05] [0] \t\t *Estimated* 816 embeddings.\n",
      "[Mar 27, 20:32:05] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/resume_4/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sub_sample = torch.load(sub_sample_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: number of training points (776) is less than the minimum recommended (2560)\n",
      "used 6 iterations (0.007s) to cluster 776 items into 256 clusters\n",
      "[0.034, 0.031, 0.035, 0.033, 0.026, 0.036, 0.032, 0.039, 0.036, 0.027, 0.032, 0.045, 0.03, 0.028, 0.034, 0.034, 0.032, 0.039, 0.033, 0.036, 0.032, 0.044, 0.036, 0.03, 0.029, 0.034, 0.039, 0.028, 0.042, 0.029, 0.039, 0.04, 0.04, 0.036, 0.03, 0.03, 0.025, 0.041, 0.04, 0.038, 0.037, 0.029, 0.037, 0.031, 0.032, 0.038, 0.028, 0.034, 0.032, 0.038, 0.027, 0.036, 0.034, 0.032, 0.032, 0.041, 0.03, 0.031, 0.032, 0.039, 0.035, 0.033, 0.029, 0.032, 0.044, 0.036, 0.042, 0.039, 0.034, 0.029, 0.033, 0.026, 0.031, 0.034, 0.028, 0.032, 0.041, 0.039, 0.03, 0.032, 0.031, 0.035, 0.024, 0.033, 0.034, 0.037, 0.038, 0.033, 0.031, 0.032, 0.033, 0.03, 0.03, 0.041, 0.029, 0.027, 0.04, 0.028, 0.031, 0.037, 0.03, 0.036, 0.032, 0.037, 0.036, 0.028, 0.025, 0.036, 0.032, 0.025, 0.038, 0.033, 0.036, 0.04, 0.034, 0.036, 0.041, 0.039, 0.033, 0.028, 0.028, 0.027, 0.035, 0.042, 0.039, 0.034, 0.032, 0.03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 27, 20:32:05] [0] \t\t #> Encoding 6 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.20it/s]\u001b[A\n",
      "1it [00:00,  6.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2064.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 27, 20:32:05] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Mar 27, 20:32:05] #> Building the emb2pid mapping..\n",
      "[Mar 27, 20:32:05] len(emb2pid) = 816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:00<00:00, 80118.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 27, 20:32:05] #> Saved optimized IVF to .ragatouille/colbert/indexes/resume_4/ivf.pid.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.ragatouille/colbert/indexes/resume_4'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_text_4 = df_resumes[df_resumes[\"resume_id\"] == 4][\"resume_text\"].iloc[0]\n",
    "\n",
    "RAG.index(\n",
    "    collection=[resume_text_4],\n",
    "    index_name=\"resume_4\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82675c4d-1286-41cd-b839-5885d08bd336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New index_name received! Updating current index_name (resume_4) to resume_5\n",
      "Loading searcher for index resume_5 for the first time... This may take a few seconds\n",
      "[Mar 27, 20:32:28] #> Loading codec...\n",
      "[Mar 27, 20:32:28] #> Loading IVF...\n",
      "[Mar 27, 20:32:28] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3221.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 27, 20:32:28] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1198.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Mathematics, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([ 101,    1, 5597,  102,  103,  103,  103,  103,  103,  103,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': 'Reports and Forecasts Education Details PGP in Data Science Mumbai, Maharashtra Aegis School of data science & Business B. E. in Electronics & Communication Electronics & Communication Indore, Madhya Pradesh IES IPS Academy Data Scientist Data Scientist with PR Canada Skill Details Algorithms- Exprience - 6 months BI- Exprience - 6 months Business Intelligence- Exprience - 6 months Machine Learning- Exprience - 24 months Visualization- Exprience - 24 months spark- Exprience - 24 months python- Exprience - 36 months tableau- Exprience - 36 months Data Analysis- Exprience - 24 monthsCompany Details company - Aegis school of Data Science & Business description - Mostly working on industry project for providing solution along with Teaching Appointments: Teach undergraduate and graduate-level courses in Spark and Machine Learning as an adjunct faculty member at Aegis School of Data Science,',\n",
       "  'score': 11.318258285522461,\n",
       "  'rank': 1,\n",
       "  'document_id': 'c00c7312-edb5-4962-a355-3f908dba3fe7',\n",
       "  'passage_id': 1},\n",
       " {'content': 'Expertise Data and Quantitative Analysis Decision Analytics Predictive Modeling Data-Driven Personalization KPI Dashboards Big Data Queries and Interpretation Data Mining and Visualization Tools Machine Learning Algorithms Business Intelligence ( BI ) Research, Reports and Forecasts Education Details PGP in Data Science Mumbai, Maharashtra Aegis School of data science & Business B. E. in Electronics & Communication Electronics & Communication Indore,',\n",
       "  'score': 5.1642537117004395,\n",
       "  'rank': 2,\n",
       "  'document_id': 'c00c7312-edb5-4962-a355-3f908dba3fe7',\n",
       "  'passage_id': 0},\n",
       " {'content': 'Mumbai ( 2017 to Present ) company - Aegis school of Data & Business description - Data Science Intern, Nov 2015 to Jan 2016 Furnish executive leadership team with insights, analytics, reports and recommendations enabling effective strategic planning across all business units, distribution channels and product lines. Chat Bot using AWS LEX and Tensor flow Python The goal of project creates a chat bot for an academic institution or university to handle queries related courses offered by that institute. The objective of this task is to reduce human efforts as well as reduce man made errors. Even by this companies handle their client 24x7. In this case companies are academic institutions and clients are participants or students. Web scraping using Selenium web driver Python The task is to scrap the data from the online messaging portal in a text format and have to find the pattern form it.',\n",
       "  'score': 3.494795322418213,\n",
       "  'rank': 3,\n",
       "  'document_id': 'c00c7312-edb5-4962-a355-3f908dba3fe7',\n",
       "  'passage_id': 2}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG.search(query=\"Mathematics\", index_name=\"resume_5\", k=3) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
