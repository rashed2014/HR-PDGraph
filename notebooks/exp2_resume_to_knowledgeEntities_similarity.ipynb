{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa03d54d-1321-4350-ba36-410cec567dc8",
   "metadata": {},
   "source": [
    "## üéØ Resume-to-O*NET Knowledge Mapping (High-Quality Annotations)\n",
    "\n",
    "### Purpose\n",
    "This notebook demonstrates a focused exploration of how high-quality resumes align with the **O*NET Knowledge taxonomy** using semantic similarity techniques.\n",
    "\n",
    "> ‚úÖ This exploration is based on:\n",
    "> - **Objective 1**\n",
    "> - **Annotations Set 1**\n",
    "> - Resumes with a **rating score of 5**\n",
    "\n",
    "The goal is to:\n",
    "- Extract meaningful **noun phrases** from resume text\n",
    "- Match them semantically to relevant **O*NET knowledge entities**\n",
    "- Assess the **semantic similarity** between the resume content and standardized knowledge requirements\n",
    "- Compare against the **importance (`data_value`)** of those knowledge areas for a given job role\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Methodology\n",
    "\n",
    "1. **Data Selection**  \n",
    "   - Resumes are queried from a SQLite database where the annotation rating is 5.\n",
    "   - Each resume is associated with a predicted job title.\n",
    "\n",
    "2. **Text Processing**  \n",
    "   - `TextBlob` is used to extract **noun phrases** from each resume, replicating the method described in the referenced research.\n",
    "\n",
    "3. **Knowledge Entity Matching**  \n",
    "   - For each resume, we select the corresponding **O*NET knowledge entities** by fuzzy-matching the job title to the `job_title` column in the O*NET knowledge dataset.\n",
    "\n",
    "4. **Embedding & Similarity Scoring**  \n",
    "   - Both noun phrases and knowledge entities are encoded using the `all-MiniLM-L6-v2` **SentenceTransformer**.\n",
    "   - **Cosine similarity** is calculated between each noun phrase and each knowledge entity.\n",
    "   - All matches with **similarity ‚â• 0.65** are retained.\n",
    "\n",
    "5. **Output Construction**  \n",
    "   - For each `(noun phrase, knowledge entity)` pair, the following are recorded:\n",
    "     - Resume ID\n",
    "     - Job Title\n",
    "     - Noun Phrase\n",
    "     - O*NET Knowledge Entity\n",
    "     - Cosine Similarity Score\n",
    "     - O*NET `data_value` (importance level of the knowledge for that job)\n",
    "\n",
    "---\n",
    "\n",
    "### Outcome\n",
    "The output DataFrame provides an interpretable mapping between **resume content and job-specific knowledge requirements**. It enables:\n",
    "- Visualizing **how well a resume covers the most important knowledge areas** for a job\n",
    "- Exploring **semantic overlap** between applicant experience and formal occupation standards\n",
    "- Evaluating entity alignment at a **granular, phrase-level resolution**\n",
    "\n",
    "This exploration supports downstream use cases like:\n",
    "- Automated resume-job fit scoring\n",
    "- Gap analysis between candidate skills and job requirements\n",
    "- Training data analysis for classification models\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "Alonso, R., Dess√≠, D., Meloni, A., & Reforgiato Recupero, D. (2025).  \n",
    "**A novel approach for job matching and skill recommendation using transformers and the O\\*NET database**.  \n",
    "*Big Data Research, 39*, 100509. [https://www.sciencedirect.com/science/article/pii/S2214579625000048)\n",
    "\n",
    "This notebook replicates and adapts core techniques from the above paper, including noun phrase extraction with TextBlob and semantic similarity scoring against O\\*NET entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb0d1ca-db12-4faa-bace-800666ee5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package conll2000 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3dc6522-98d2-436c-8a49-2a985826daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   resume_id                              job_title  \\\n",
      "0          4                   Computer Programmers   \n",
      "1          4                   Computer Programmers   \n",
      "2          4                   Computer Programmers   \n",
      "3          4                   Computer Programmers   \n",
      "4          4  Computer Systems Engineers/Architects   \n",
      "\n",
      "                 noun_phrase           knowledge_entity  similarity_score  \\\n",
      "0  communication electronics  Computers and Electronics          0.679181   \n",
      "1  communication electronics  Computers and Electronics          0.679181   \n",
      "2                electronics  Computers and Electronics          0.840779   \n",
      "3                electronics  Computers and Electronics          0.840779   \n",
      "4  communication electronics  Computers and Electronics          0.679181   \n",
      "\n",
      "   data_value  \n",
      "0        4.87  \n",
      "1        6.16  \n",
      "2        4.87  \n",
      "3        6.16  \n",
      "4        4.91  \n",
      "‚úÖ Done computing similarity based on noun phrases.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load the embedding model\n",
    "import torch\n",
    "\n",
    "# Set the target GPU (e.g., GPU 0 or GPU 1)\n",
    "device = torch.device(\"cuda:0\")  # or \"cuda:1\" for the second GPU\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = \"../data/annotations_scenario_1/annotations_scenario_1.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Step 1: Query 10 resumes with rating = 5\n",
    "query_resumes = \"\"\"\n",
    "SELECT r.id AS resume_id, r.resume_text, pj.job_title\n",
    "FROM resumes r\n",
    "JOIN annotations a ON r.id = a.resume_id\n",
    "JOIN predicted_jobs pj ON r.id = pj.resume_id\n",
    "WHERE a.rating = 5;\n",
    "\"\"\"\n",
    "df_resumes = pd.read_sql_query(query_resumes, conn)\n",
    "conn.close()\n",
    "\n",
    "# Step 2: Load the O*NET knowledge dataset (previously processed)\n",
    "df_onet = pd.read_csv(\"../data/annotations_scenario_1/processed_onet_knowledge.csv\")\n",
    "\n",
    "# Step 3: Initialize a list to collect similarity results\n",
    "similarity_results = []\n",
    "\n",
    "# Step 4: Iterate over resumes\n",
    "for _, row in df_resumes.iterrows():\n",
    "    resume_id = row[\"resume_id\"]\n",
    "    resume_text = row[\"resume_text\"]\n",
    "    job_title = row[\"job_title\"]\n",
    "\n",
    "    # Get knowledge entities for this job title using pattern matching\n",
    "    df_knowledge = df_onet[df_onet[\"job_title\"].str.contains(job_title, case=False, na=False, regex=True)]\n",
    "\n",
    "    if df_knowledge.empty:\n",
    "        print(f\"‚ö†Ô∏è No knowledge entities found for job: {job_title} (Resume ID: {resume_id})\")\n",
    "        continue\n",
    "\n",
    "    # Extract noun phrases from the resume\n",
    "    blob = TextBlob(resume_text)\n",
    "    noun_phrases = list(set(blob.noun_phrases))  # Remove duplicates\n",
    "\n",
    "    if not noun_phrases:\n",
    "        print(f\"‚ö†Ô∏è No noun phrases found in resume ID {resume_id}\")\n",
    "        continue\n",
    "\n",
    "    # Encode noun phrases and knowledge entities\n",
    "    resume_embeddings = model.encode(noun_phrases, convert_to_numpy=True)\n",
    "    knowledge_entities = df_knowledge[\"knowledge_entity\"].tolist()\n",
    "    knowledge_embeddings = model.encode(knowledge_entities, convert_to_numpy=True)\n",
    "\n",
    "    # Compute pairwise cosine similarity\n",
    "    similarity_matrix = cosine_similarity(resume_embeddings, knowledge_embeddings)\n",
    "\n",
    "    # Store all similarity scores ‚â• 0.65, and include data_value from df_knowledge\n",
    "    threshold = 0.65\n",
    "    for i, noun_phrase in enumerate(noun_phrases):\n",
    "        for j, knowledge_entity in enumerate(knowledge_entities):\n",
    "            score = similarity_matrix[i, j]\n",
    "            if score >= threshold:\n",
    "                data_value = df_knowledge.iloc[j][\"data_value\"]\n",
    "                similarity_results.append({\n",
    "                    \"resume_id\": resume_id,\n",
    "                    \"job_title\": job_title,\n",
    "                    \"noun_phrase\": noun_phrase,\n",
    "                    \"knowledge_entity\": knowledge_entity,\n",
    "                    \"similarity_score\": score,\n",
    "                    \"data_value\": data_value\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_similarity = pd.DataFrame(similarity_results).drop_duplicates()\n",
    "\n",
    "# Display the similarity results\n",
    "print(df_similarity.head())\n",
    "\n",
    "# (Optional) Save to CSV\n",
    "# df_similarity.to_csv(\"resume_knowledge_similarity_matrix.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Done computing similarity based on noun phrases.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ca5837-c5e0-469e-b304-13c35d447d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>235</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>315</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>225</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     resume_id  count\n",
       "0           27    140\n",
       "1          132    140\n",
       "2          276     80\n",
       "3          171     80\n",
       "4           66     80\n",
       "..         ...    ...\n",
       "124        235     10\n",
       "125        315     10\n",
       "126        120      9\n",
       "127        225      9\n",
       "128         15      9\n",
       "\n",
       "[129 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity[\"resume_id\"].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6405f-85c1-466e-800f-f43be8a909ba",
   "metadata": {},
   "source": [
    "# Lets connect to neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fc58c-b5c2-49fa-98c8-8c161ce1b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0100c7c5-eaaf-46ae-abc2-e9de4b65a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Replace with your Neo4j URI and credentials\n",
    "NEO4J_URI = \"bolt://20.14.162.151:7687\" \n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"recluse2025\"\n",
    "\n",
    "# Connect to Neo4j\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# Transaction function to push nodes and relationships\n",
    "def push_to_neo4j(tx, record):\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (r:Resume {id: $resume_id})\n",
    "        SET r.job_title = $job_title\n",
    "\n",
    "        MERGE (n:NounPhrase {text: $noun_phrase})\n",
    "        MERGE (k:Knowledge {entity: $knowledge_entity})\n",
    "        MERGE (j:JobTitle {title: $job_title})\n",
    "\n",
    "        MERGE (r)-[:CONTAINS]->(n)\n",
    "        MERGE (n)-[s:SIMILAR_TO]->(k)\n",
    "        SET s.score = $similarity_score\n",
    "\n",
    "        MERGE (k)-[rj:REQUIRED_FOR]->(j)\n",
    "        SET rj.importance = $data_value\n",
    "    \"\"\", \n",
    "    resume_id=record[\"resume_id\"],\n",
    "    job_title=record[\"job_title\"],\n",
    "    noun_phrase=record[\"noun_phrase\"],\n",
    "    knowledge_entity=record[\"knowledge_entity\"],\n",
    "    similarity_score=round(record[\"similarity_score\"], 4),\n",
    "    data_value=record[\"data_value\"])\n",
    "\n",
    "\n",
    "# Push rows from DataFrame to Neo4j\n",
    "with driver.session() as session:\n",
    "    for _, row in df_similarity.iterrows():\n",
    "        session.execute_write(push_to_neo4j, row)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee92726b-1408-4695-9e63-96a93516aba1",
   "metadata": {},
   "source": [
    "### üìä Graph Cluster: Computer Programmer\n",
    "\n",
    "![Computer Programmer Cluster](../images/graph.png)\n",
    "\n",
    "> **Figure**: Cluster of resumes linked to the *Computer Programmer* job title via shared knowledge entities and noun phrases extracted from annotated resumes.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Cluster Overview\n",
    "\n",
    "This graph visualizes the semantic relationship between:\n",
    "\n",
    "- **Resumes** ‚Äî job applicant profiles\n",
    "- **Noun phrases** ‚Äî extracted from resume text using NLP (e.g., `\"data structures\"`, `\"software development\"`)\n",
    "- **Knowledge entities** ‚Äî from O*NET's taxonomy (e.g., `\"Programming\"`, `\"Computers and Electronics\"`)\n",
    "- **Job title** ‚Äî `\"Computer Programmer\"`\n",
    "\n",
    "Each relationship is **weighted**, capturing:\n",
    "- `SIMILAR_TO.score`: how semantically similar a noun phrase is to a knowledge concept\n",
    "- `REQUIRED_FOR.importance`: how important that knowledge is for the job (from O*NET)\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Why This Graph Helps\n",
    "\n",
    "- ‚úÖ **Semantic Grouping**: Resumes are grouped by *meaning*, not just keywords.\n",
    "- üîç **Explainability**: We can trace *why* a resume links to a job through specific skills and phrases.\n",
    "- üåê **Community Detection**: Similar resumes cluster around shared knowledge, enabling skill-based profile comparison.\n",
    "- üîÅ **Dynamic Querying**:\n",
    "  - Find resumes aligned to a job\n",
    "  - Discover knowledge areas across resumes\n",
    "  - Surface alternate roles with overlapping skill needs\n",
    "  - Find which knowledge, skills and etc. are bringing resumes closer - e.g. knowledge with high centrality\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Use Case Potential\n",
    "\n",
    "- Intelligent resume screening\n",
    "- Career path exploration for candidates\n",
    "- Training needs detection\n",
    "- AI-driven workforce planning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
