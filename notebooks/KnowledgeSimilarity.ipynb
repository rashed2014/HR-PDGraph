{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf8bbd0-887e-4df7-a9f7-3ef3ba5a688a",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "The research paper uses sentence embedding on noun and noun phrases. This analysis is to see of other modern approaches can reach same or better score. Ultimately, we want to see if the new approach at least mataches the high scores annotated. \n",
    "\n",
    "1. Compare embedding of entire resume to individual entities of a category. Expect sim,ilarity to be less than the research paper\n",
    "2. Colbert index and search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c245f36-49ba-4ef3-b96f-1e681f581b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1b484-7248-4200-bcca-88aa2cd38bdc",
   "metadata": {},
   "source": [
    "### Run the following code for O*Net Knowledge Excel to CSV\n",
    "\n",
    "```python\n",
    "###### Load the O*NET Knowledge Excel file\n",
    "knowledge_file = \"data/annotations_scenario_1/Knowledge.xlsx\"  # Update with the actual filename\n",
    "df_onet = pd.read_excel(knowledge_file)\n",
    "\n",
    "##### Select relevant columns\n",
    "df_onet = df_onet[[\"O*NET-SOC Code\", \"Title\", \"Element Name\", \"Scale ID\", \"Data Value\"]]\n",
    "\n",
    "##### Filter for only importance (IM) and level (LV)\n",
    "df_onet = df_onet[df_onet[\"Scale ID\"].isin([\"IM\", \"LV\"])]\n",
    "\n",
    "##### Rename columns for consistency\n",
    "df_onet.rename(columns={\n",
    "    \"O*NET-SOC Code\": \"onetsoc_code\",\n",
    "    \"Title\": \"job_title\",\n",
    "    \"Element Name\": \"knowledge_entity\",\n",
    "    \"Scale ID\": \"scale_id\",\n",
    "    \"Data Value\": \"data_value\"\n",
    "}, inplace=True)\n",
    "\n",
    "#### Display the processed data in a Pandas DataFrame\n",
    "print(df_onet.head())  # Show the first few rows\n",
    "\n",
    "#### Save to CSV if you want to inspect it further\n",
    "df_onet.to_csv(\"data/annotations_scenario_1/processed_onet_knowledge.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69182a26-6e2b-4504-82bf-750898392c37",
   "metadata": {},
   "source": [
    "### Run the following to convert O*Net Occupation Excel to CSV\n",
    "\n",
    "```python\n",
    "# Load the O*NET Knowledge Excel file\n",
    "occupation_file = \"data/annotations_scenario_1/Occupation Data.xlsx\"  # Update with the actual filename\n",
    "\n",
    "df_occupation = pd.read_excel(occupation_file)\n",
    "\n",
    "# Select relevant columns\n",
    "df_occupation = df_occupation[[\"O*NET-SOC Code\", \"Title\", \"Description\"]]\n",
    "\n",
    "# Rename columns for consistency\n",
    "df_occupation.rename(columns={\n",
    "    \"O*NET-SOC Code\": \"onetsoc_code\",\n",
    "    \"Title\": \"job_title\",\n",
    "    \"Description\": \"job_description\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_occupation.head())\n",
    "\n",
    "# Save to CSV for further inspection (optional)\n",
    "df_occupation.to_csv(\"data/annotations_scenario_1/processed_onet_occupation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c597265-179e-4ae6-9c41-e8da6c9e9563",
   "metadata": {},
   "source": [
    "### create annotions database\n",
    "```python\n",
    "!sqlite3 ../data/annotations_scenario_1/annotations_scenario_1.db < ../data/annotations_scenario_1/annotations_scenario_1.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8edfa01d-5375-4dff-b9e1-462d2492455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   resume_id             job_title               knowledge_entity  \\\n",
      "0          4  Computer Programmers  Administration and Management   \n",
      "2          4  Computer Programmers                 Administrative   \n",
      "4          4  Computer Programmers       Economics and Accounting   \n",
      "6          4  Computer Programmers            Sales and Marketing   \n",
      "8          4  Computer Programmers  Customer and Personal Service   \n",
      "\n",
      "   similarity_score  \n",
      "0          0.309047  \n",
      "2          0.231593  \n",
      "4          0.223551  \n",
      "6          0.269907  \n",
      "8          0.232240  \n",
      "✅ Similarity matrix saved as 'resume_knowledge_similarity_matrix.csv'.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(\"../data/annotations_scenario_1/annotations_scenario_1.db\")\n",
    "\n",
    "# Step 1: Query 10 resumes with rating = 5\n",
    "query_resumes = \"\"\"\n",
    "SELECT r.id AS resume_id, r.resume_text, pj.job_title\n",
    "FROM resumes r\n",
    "JOIN annotations a ON r.id = a.resume_id\n",
    "JOIN predicted_jobs pj ON r.id = pj.resume_id\n",
    "WHERE a.rating = 5\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "df_resumes = pd.read_sql_query(query_resumes, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Step 2: Load the O*NET knowledge dataset (previously processed)\n",
    "df_onet = pd.read_csv(\"../data/annotations_scenario_1/processed_onet_knowledge.csv\")\n",
    "\n",
    "# Step 3: Initialize an empty list to store similarity results\n",
    "similarity_results = []\n",
    "\n",
    "# Step 4: Compute similarity for each resume and its corresponding job knowledge entities\n",
    "for _, row in df_resumes.iterrows():\n",
    "    resume_id = row[\"resume_id\"]\n",
    "    resume_text = row[\"resume_text\"]\n",
    "    job_title = row[\"job_title\"]\n",
    "\n",
    "    # Get knowledge entities for this job title\n",
    "    df_knowledge = df_onet[df_onet[\"job_title\"] == job_title]\n",
    "\n",
    "    if df_knowledge.empty:\n",
    "        print(f\"⚠️ No knowledge entities found for job: {job_title} (Resume ID: {resume_id})\")\n",
    "        continue  # Skip if no knowledge data exists for this job\n",
    "\n",
    "    # Generate embeddings\n",
    "    resume_embedding = model.encode(resume_text, convert_to_numpy=True)\n",
    "    knowledge_embeddings = df_knowledge[\"knowledge_entity\"].apply(lambda x: model.encode(x, convert_to_numpy=True))\n",
    "\n",
    "    # Compute similarity\n",
    "    similarity_scores = cosine_similarity([resume_embedding], list(knowledge_embeddings))\n",
    "\n",
    "    # Store results\n",
    "    for knowledge_entity, score in zip(df_knowledge[\"knowledge_entity\"], similarity_scores[0]):\n",
    "        similarity_results.append({\"resume_id\": resume_id, \"job_title\": job_title, \"knowledge_entity\": knowledge_entity, \"similarity_score\": score})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_similarity = pd.DataFrame(similarity_results)\n",
    "\n",
    "# Remove duplicates if any remain\n",
    "df_similarity.drop_duplicates(inplace=True)\n",
    "\n",
    "# Print a preview of the similarity matrix\n",
    "print(df_similarity.head())\n",
    "\n",
    "# Save to CSV for further analysis\n",
    "df_similarity.to_csv(\"../data/annotations_scenario_1/resume_knowledge_similarity_matrix.csv\", index=False)\n",
    "\n",
    "print(\"✅ Similarity matrix saved as 'resume_knowledge_similarity_matrix.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1a2a76-0037-49f3-aeb9-9a039b95b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Highest Similarity Score:\n",
      "     resume_id                                     job_title  \\\n",
      "346          5                   Computer Network Architects   \n",
      "412          5                          Computer Programmers   \n",
      "478          5         Computer Systems Engineers/Architects   \n",
      "544          5  Computer and Information Research Scientists   \n",
      "610          5                            Robotics Engineers   \n",
      "\n",
      "              knowledge_entity  similarity_score  \n",
      "346  Computers and Electronics          0.418219  \n",
      "412  Computers and Electronics          0.418219  \n",
      "478  Computers and Electronics          0.418219  \n",
      "544  Computers and Electronics          0.418219  \n",
      "610  Computers and Electronics          0.418219  \n"
     ]
    }
   ],
   "source": [
    "# Print the highest similarity score\n",
    "max_similarity = df_similarity[\"similarity_score\"].max()\n",
    "highest_match = df_similarity[df_similarity[\"similarity_score\"] == max_similarity]\n",
    "\n",
    "print(\"\\n🎯 Highest Similarity Score:\")\n",
    "print(highest_match)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce8b0a51-9668-4f0b-a8ba-15b0bf776852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 19:14:00] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/opt/conda/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "# Load ColBERT-based RAG model from Ragatouille\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7919b5fe-b5e4-447e-ad37-b394b2222d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Resume ID: 4...\n",
      "\n",
      "\n",
      "[Mar 28, 19:14:45] #> Creating directory .ragatouille/colbert/indexes/resume_4 \n",
      "\n",
      "\n",
      "[Mar 28, 19:14:45] [0] \t\t #> Encoding 6 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/opt/conda/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "/opt/conda/lib/python3.11/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 19:14:46] [0] \t\t avg_doclen_est = 136.0 \t len(local_sample) = 6\n",
      "[Mar 28, 19:14:46] [0] \t\t Creating 256 partitions.\n",
      "[Mar 28, 19:14:46] [0] \t\t *Estimated* 816 embeddings.\n",
      "[Mar 28, 19:14:46] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/resume_4/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sub_sample = torch.load(sub_sample_path)\n",
      "WARNING clustering 776 points to 256 centroids: please provide at least 9984 training points\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  centroids = torch.load(centroids_path, map_location='cpu')\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  avg_residual = torch.load(avgresidual_path, map_location='cpu')\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bucket_cutoffs, bucket_weights = torch.load(buckets_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 776 points in 128D to 256 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "[0.026, 0.035, 0.024, 0.038, 0.02, 0.037, 0.038, 0.028, 0.034, 0.028, 0.03, 0.036, 0.03, 0.029, 0.039, 0.036, 0.026, 0.043, 0.035, 0.038, 0.035, 0.037, 0.037, 0.025, 0.028, 0.035, 0.043, 0.03, 0.043, 0.037, 0.032, 0.048, 0.046, 0.037, 0.036, 0.032, 0.026, 0.038, 0.041, 0.032, 0.037, 0.039, 0.039, 0.033, 0.038, 0.037, 0.025, 0.036, 0.038, 0.043, 0.032, 0.041, 0.041, 0.029, 0.031, 0.036, 0.022, 0.031, 0.035, 0.039, 0.031, 0.034, 0.041, 0.032, 0.043, 0.036, 0.042, 0.03, 0.033, 0.033, 0.031, 0.033, 0.029, 0.035, 0.032, 0.03, 0.035, 0.039, 0.03, 0.032, 0.03, 0.036, 0.034, 0.031, 0.032, 0.035, 0.03, 0.034, 0.034, 0.03, 0.031, 0.037, 0.034, 0.034, 0.033, 0.022, 0.032, 0.036, 0.041, 0.042, 0.036, 0.04, 0.038, 0.032, 0.034, 0.025, 0.026, 0.037, 0.034, 0.026, 0.037, 0.032, 0.036, 0.04, 0.037, 0.033, 0.038, 0.039, 0.031, 0.029, 0.021, 0.029, 0.031, 0.034, 0.032, 0.032, 0.032, 0.034]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 19:14:46] [0] \t\t #> Encoding 6 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.57it/s]\u001b[A\n",
      "1it [00:00,  5.33it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual_embeddings.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(codes_path, map_location='cpu')\n",
      "100%|██████████| 1/1 [00:00<00:00, 1773.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 19:14:46] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Mar 28, 19:14:46] #> Building the emb2pid mapping..\n",
      "[Mar 28, 19:14:46] len(emb2pid) = 816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 256/256 [00:00<00:00, 75765.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 19:14:46] #> Saved optimized IVF to .ragatouille/colbert/indexes/resume_4/ivf.pid.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing!\n",
      "Indexing Resume ID: 5...\n",
      "New index_name received! Updating current index_name (resume_4) to resume_5\n",
      "\n",
      "\n",
      "[Mar 28, 19:14:46] #> Creating directory .ragatouille/colbert/indexes/resume_5 \n",
      "\n",
      "\n",
      "[Mar 28, 19:14:47] [0] \t\t #> Encoding 3 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 19:14:47] [0] \t\t avg_doclen_est = 131.0 \t len(local_sample) = 3\n",
      "[Mar 28, 19:14:47] [0] \t\t Creating 256 partitions.\n",
      "[Mar 28, 19:14:47] [0] \t\t *Estimated* 393 embeddings.\n",
      "[Mar 28, 19:14:47] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/resume_5/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING clustering 374 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering 374 points in 128D to 256 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "[0.035, 0.033, 0.044, 0.049, 0.043, 0.033, 0.04, 0.038, 0.05, 0.027, 0.036, 0.03, 0.047, 0.028, 0.037, 0.037, 0.044, 0.043, 0.03, 0.045, 0.031, 0.029, 0.04, 0.031, 0.03, 0.036, 0.041, 0.04, 0.041, 0.052, 0.035, 0.032, 0.037, 0.033, 0.045, 0.045, 0.034, 0.021, 0.035, 0.047, 0.043, 0.047, 0.047, 0.04, 0.038, 0.03, 0.037, 0.033, 0.048, 0.042, 0.03, 0.051, 0.046, 0.049, 0.038, 0.036, 0.034, 0.039, 0.028, 0.045, 0.032, 0.061, 0.051, 0.038, 0.036, 0.041, 0.034, 0.034, 0.043, 0.033, 0.035, 0.044, 0.031, 0.036, 0.028, 0.039, 0.046, 0.05, 0.039, 0.046, 0.037, 0.03, 0.063, 0.034, 0.035, 0.035, 0.044, 0.045, 0.054, 0.032, 0.041, 0.046, 0.048, 0.039, 0.041, 0.042, 0.036, 0.043, 0.035, 0.035, 0.031, 0.045, 0.03, 0.047, 0.041, 0.026, 0.026, 0.033, 0.046, 0.028, 0.047, 0.035, 0.031, 0.043, 0.046, 0.041, 0.035, 0.045, 0.046, 0.023, 0.054, 0.026, 0.039, 0.038, 0.036, 0.037, 0.033, 0.031]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 19:14:47] [0] \t\t #> Encoding 3 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "1it [00:00, 11.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2465.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 19:14:47] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Mar 28, 19:14:47] #> Building the emb2pid mapping..\n",
      "[Mar 28, 19:14:47] len(emb2pid) = 393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 256/256 [00:00<00:00, 88315.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 19:14:47] #> Saved optimized IVF to .ragatouille/colbert/indexes/resume_5/ivf.pid.pt\n",
      "Done indexing!\n",
      "✅ All resumes have been indexed successfully (only once per ID)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Group by resume_id to ensure one resume per ID\n",
    "df_resumes_grouped = df_resumes.groupby(\"resume_id\")[\"resume_text\"].first().reset_index()\n",
    "\n",
    "# Step 2: Index Each Resume Independently\n",
    "for resume_id, resume_text in zip(df_resumes_grouped[\"resume_id\"], df_resumes_grouped[\"resume_text\"]):\n",
    "    index_name = f\"resume_{resume_id}\"  # Unique index name per resume\n",
    "    \n",
    "    print(f\"Indexing Resume ID: {resume_id}...\")  # Debugging output\n",
    "    RAG.index(\n",
    "        collection=[resume_text],  # Store the full resume as a single document\n",
    "        index_name=index_name,\n",
    "        max_document_length=180,\n",
    "        split_documents=True,  # Ragatouille will handle chunking\n",
    "        use_faiss=True\n",
    "    )\n",
    "\n",
    "print(\"✅ All resumes have been indexed successfully (only once per ID)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a164c11-a75f-4794-b702-86c508fcfd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resumes[\"resume_id\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26addeee-826b-4de9-bae5-3d8a7378e058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching Resume ID: 4...\n",
      "New index_name received! Updating current index_name (resume_5) to resume_4\n",
      "Loading searcher for index resume_4 for the first time... This may take a few seconds\n",
      "[Mar 28, 20:42:32] #> Loading codec...\n",
      "[Mar 28, 20:42:32] #> Loading IVF...\n",
      "[Mar 28, 20:42:32] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/opt/conda/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  centroids = torch.load(centroids_path, map_location='cpu')\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  avg_residual = torch.load(avgresidual_path, map_location='cpu')\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bucket_cutoffs, bucket_weights = torch.load(buckets_path, map_location='cpu')\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/search/index_loader.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ivf, ivf_lengths = torch.load(os.path.join(self.index_path, \"ivf.pid.pt\"), map_location='cpu')\n",
      "100%|██████████| 1/1 [00:00<00:00, 5940.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 20:42:32] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual_embeddings.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(codes_path, map_location='cpu')\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual_embeddings.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(residuals_path, map_location='cpu')\n",
      "100%|██████████| 1/1 [00:00<00:00, 760.25it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "/opt/conda/lib/python3.11/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Administration and Management, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([ 101,    1, 3447, 1998, 2968,  102,  103,  103,  103,  103,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "🔍 Searching Resume ID: 5...\n",
      "New index_name received! Updating current index_name (resume_4) to resume_5\n",
      "Loading searcher for index resume_5 for the first time... This may take a few seconds\n",
      "[Mar 28, 20:42:38] #> Loading codec...\n",
      "[Mar 28, 20:42:38] #> Loading IVF...\n",
      "[Mar 28, 20:42:38] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3446.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 28, 20:42:38] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 1232.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Administration and Management, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([ 101,    1, 3447, 1998, 2968,  102,  103,  103,  103,  103,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Highest Raw Similarity Score using Ragatouille:\n",
      "     resume_id                       query  \\\n",
      "125          5  Engineering and Technology   \n",
      "\n",
      "                                  matched_resume_chunk  similarity_score  \n",
      "125  Reports and Forecasts Education Details PGP in...           16.3687  \n",
      "✅ Similarity matrix saved as 'ragatouille_resume_knowledge_similarity_matrix.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Step 1: Prepare queries (all unique O*NET knowledge entities)\n",
    "knowledge_queries = df_onet[\"knowledge_entity\"].drop_duplicates().tolist()\n",
    "\n",
    "# Step 2: Search for each indexed resume\n",
    "similarity_results = []\n",
    "for resume_id in df_resumes[\"resume_id\"].unique():\n",
    "    index_name = f\"resume_{resume_id}\"  # Resume index name\n",
    "    print(f\"🔍 Searching Resume ID: {resume_id}...\")  # Debugging output\n",
    "\n",
    "    # Iterate over each knowledge query instead of stacking them\n",
    "    for query in knowledge_queries:\n",
    "        retrieved_docs = RAG.search(query=query, index_name=index_name, k=3)  \n",
    "\n",
    "        # Store results with raw similarity scores\n",
    "        for doc in retrieved_docs:\n",
    "            matched_text = doc[\"content\"]\n",
    "            similarity_score = doc[\"score\"]  # Keep raw score\n",
    "\n",
    "            similarity_results.append({\n",
    "                \"resume_id\": resume_id,\n",
    "                \"query\": query,\n",
    "                \"matched_resume_chunk\": matched_text,\n",
    "                \"similarity_score\": round(similarity_score, 4)  # Keep raw score\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_similarity = pd.DataFrame(similarity_results)\n",
    "\n",
    "# Step 3: Find the highest similarity score (raw, unnormalized)\n",
    "if not df_similarity.empty:\n",
    "    max_similarity = df_similarity[\"similarity_score\"].max()\n",
    "    highest_match = df_similarity[df_similarity[\"similarity_score\"] == max_similarity]\n",
    "\n",
    "    print(\"\\n🎯 Highest Raw Similarity Score using Ragatouille:\")\n",
    "    print(highest_match)\n",
    "else:\n",
    "    print(\"\\n🚨 No results found.\")\n",
    "\n",
    "# Save to CSV\n",
    "df_similarity.to_csv(\"../data/annotations_scenario_1/ragatouille_resume_knowledge_similarity_matrix.csv\", index=False)\n",
    "print(\"✅ Similarity matrix saved as 'ragatouille_resume_knowledge_similarity_matrix.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac578a2b-35c1-469a-a2a7-b650bee15425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Filtered Results (Scores >= 10):\n",
      "     resume_id                          query  \\\n",
      "12           4  Customer and Personal Service   \n",
      "24           4      Computers and Electronics   \n",
      "25           4      Computers and Electronics   \n",
      "66           4         Education and Training   \n",
      "67           4         Education and Training   \n",
      "93           4       Communications and Media   \n",
      "94           4       Communications and Media   \n",
      "123          5      Computers and Electronics   \n",
      "125          5     Engineering and Technology   \n",
      "126          5     Engineering and Technology   \n",
      "135          5                    Mathematics   \n",
      "161          5         Education and Training   \n",
      "\n",
      "                                  matched_resume_chunk  similarity_score  \n",
      "12   Face recognition is the recognizing a special ...           12.3463  \n",
      "24   Expertise Data and Quantitative Analysis Decis...           11.4292  \n",
      "25   Reports and Forecasts Education Details PGP in...           10.3551  \n",
      "66   Reports and Forecasts Education Details PGP in...           11.7360  \n",
      "67   Expertise Data and Quantitative Analysis Decis...           11.4185  \n",
      "93   Reports and Forecasts Education Details PGP in...           11.1671  \n",
      "94   Expertise Data and Quantitative Analysis Decis...           10.8313  \n",
      "123  Reports and Forecasts Education Details PGP in...           12.4681  \n",
      "125  Reports and Forecasts Education Details PGP in...           16.3687  \n",
      "126  Expertise Data and Quantitative Analysis Decis...           14.2196  \n",
      "135  Reports and Forecasts Education Details PGP in...           11.2825  \n",
      "161  Expertise Data and Quantitative Analysis Decis...           12.0051  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter similarity scores >= 10\n",
    "df_filtered = df_similarity[df_similarity[\"similarity_score\"] >= 10]\n",
    "\n",
    "# Display the filtered results\n",
    "print(\"\\n🎯 Filtered Results (Scores >= 10):\")\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec831412-721b-4cfd-a734-77d53778966b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Merged Data with Job Titles:\n",
      "   resume_id                          query  \\\n",
      "0          4  Customer and Personal Service   \n",
      "1          4  Customer and Personal Service   \n",
      "2          4  Customer and Personal Service   \n",
      "3          4  Customer and Personal Service   \n",
      "4          4  Customer and Personal Service   \n",
      "\n",
      "                                matched_resume_chunk  similarity_score  \\\n",
      "0  Face recognition is the recognizing a special ...           12.3463   \n",
      "1  Face recognition is the recognizing a special ...           12.3463   \n",
      "2  Face recognition is the recognizing a special ...           12.3463   \n",
      "3  Face recognition is the recognizing a special ...           12.3463   \n",
      "4  Face recognition is the recognizing a special ...           12.3463   \n",
      "\n",
      "   data_value onetsoc_code                        job_title  \n",
      "0        4.39   11-1011.00                 Chief Executives  \n",
      "1        5.94   11-1011.00                 Chief Executives  \n",
      "2        3.41   11-1011.03    Chief Sustainability Officers  \n",
      "3        4.44   11-1011.03    Chief Sustainability Officers  \n",
      "4        4.15   11-1021.00  General and Operations Managers  \n"
     ]
    }
   ],
   "source": [
    "# Load processed O*NET knowledge data (which includes job mapping)\n",
    "df_onet_knowledge = pd.read_csv(\"../data/annotations_scenario_1/processed_onet_knowledge.csv\")\n",
    "\n",
    "# Filter similarity scores >= 10\n",
    "df_filtered = df_similarity[df_similarity[\"similarity_score\"] >= 10]\n",
    "\n",
    "# Merge similarity results with O*NET knowledge data (to get jobs)\n",
    "df_merged = df_filtered.merge(df_onet_knowledge, left_on=\"query\", right_on=\"knowledge_entity\", how=\"left\")\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_merged = df_merged[[\"resume_id\", \"query\", \"matched_resume_chunk\", \"similarity_score\", \"data_value\", \"onetsoc_code\", \"job_title\"]]\n",
    "\n",
    "# Display merged dataset\n",
    "print(\"\\n🔍 Merged Data with Job Titles:\")\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e8765ef-6f07-49fa-aa29-6f0d27c95576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 Predicted Job Scores:\n",
      "   resume_id                 job_title   job_score\n",
      "0          4  Accountants and Auditors  493.753572\n",
      "1          4                    Actors  495.708854\n",
      "2          4                 Actuaries  482.022268\n",
      "3          4            Acupuncturists  548.439976\n",
      "4          4         Acute Care Nurses  547.672771\n",
      "✅ Predicted job scores saved as 'ragatouille_predicted_jobs.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6631/1911776919.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: (x[\"similarity_score\"] * x[\"data_value\"]).sum()) \\\n"
     ]
    }
   ],
   "source": [
    "# Compute the weighted job score: Sum of (similarity_score * data_value) per job\n",
    "df_job_scores = df_merged.groupby([\"resume_id\", \"job_title\"]) \\\n",
    "    .apply(lambda x: (x[\"similarity_score\"] * x[\"data_value\"]).sum()) \\\n",
    "    .reset_index(name=\"job_score\")\n",
    "\n",
    "# Display computed job scores\n",
    "print(\"\\n🏆 Predicted Job Scores:\")\n",
    "print(df_job_scores.head())\n",
    "\n",
    "# Save to CSV\n",
    "# df_job_scores.to_csv(\"../data/annotations_scenario_1/ragatouille_predicted_jobs.csv\", index=False)\n",
    "print(\"✅ Predicted job scores saved as 'ragatouille_predicted_jobs.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "428e7c80-0da2-4b74-ac1a-b35572d316f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Final Predicted Jobs for Each Resume:\n",
      "      resume_id                                 job_title   job_score\n",
      "162           4  Computer Science Teachers, Postsecondary  781.786266\n",
      "1148          5       Engineering Teachers, Postsecondary  709.636067\n"
     ]
    }
   ],
   "source": [
    "# Find the best matching job per resume (highest job score)\n",
    "df_best_jobs = df_job_scores.loc[df_job_scores.groupby(\"resume_id\")[\"job_score\"].idxmax()]\n",
    "\n",
    "# Display the best jobs\n",
    "print(\"\\n🎯 Final Predicted Jobs for Each Resume:\")\n",
    "print(df_best_jobs)\n",
    "\n",
    "# Save to CSV\n",
    "df_best_jobs.to_csv(\"../data/annotations_scenario_1/ragatouille_final_predicted_jobs.csv\", index=False)\n",
    "# print(\"✅ Final predicted jobs saved as 'ragatouille_final_predicted_jobs.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6de7de66-c615-471a-a587-bc6102f9fe57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expertise Data and Quantitative Analysis Decision Analytics Predictive Modeling Data-Driven Personalization KPI Dashboards Big Data Queries and Interpretation Data Mining and Visualization Tools Machine Learning Algorithms Business Intelligence ( BI ) Research, Reports and Forecasts Education Details PGP in Data Science Mumbai, Maharashtra Aegis School of data science & Business B. E. in Electronics & Communication Electronics & Communication Indore, Madhya Pradesh IES IPS Academy Data Scientist Data Scientist with PR Canada Skill Details Algorithms- Exprience - 6 months BI- Exprience - 6 months Business Intelligence- Exprience - 6 months Machine Learning- Exprience - 24 months Visualization- Exprience - 24 months spark- Exprience - 24 months python- Exprience - 36 months tableau- Exprience - 36 months Data Analysis- Exprience - 24 monthsCompany Details company - Aegis school of Data Science & Business description - Mostly working on industry project for providing solution along with Teaching Appointments: Teach undergraduate and graduate-level courses in Spark and Machine Learning as an adjunct faculty member at Aegis School of Data Science, Mumbai ( 2017 to Present ) company - Aegis school of Data & Business description - Data Science Intern, Nov 2015 to Jan 2016 Furnish executive leadership team with insights, analytics, reports and recommendations enabling effective strategic planning across all business units, distribution channels and product lines. Chat Bot using AWS LEX and Tensor flow Python The goal of project creates a chat bot for an academic institution or university to handle queries related courses offered by that institute. The objective of this task is to reduce human efforts as well as reduce man made errors. Even by this companies handle their client 24x7. In this case companies are academic institutions and clients are participants or students. Web scraping using Selenium web driver Python The task is to scrap the data from the online messaging portal in a text format and have to find the pattern form it. Data Visualization and Data insights Hadoop Eco System, Hive, PySpark, QlikSense The goal of this project is to build a Business Solutions to a Internet Service Provider Company, like handling data which is generated per day basis, for that we have to visualize that data and find the usage pattern form it and have a generate a reports. Image Based Fraud Detection Microsoft Face API, PySpark, Open CV The main goal of project is Recognize similarity for a face to given Database images. Face recognition is the recognizing a special face from set of different faces. Face is extracted and then compared with the database Image if that Image recognized then the person already applied for loan from somewhere else and now hiding his or her identity, this is how we are going to prevent the frauds in the initial stage itself. Churn Analysis for Internet Service Provider R, Python, Machine Learning, Hadoop The objective is to identify the customer who is likely to churn in a given period of time; we have to pretend the customer giving incentive offers. Sentiment Analysis Python, NLP, Apache Spark service in IBM Bluemix. This project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a sentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions of the tweets and comparative analysis. Quantifiable Results: Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science and Business Analytics. Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference called Data Science Congress by Aegis School of Business Mumbai. Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai and managed 4-5 live projects using Data Science techniques. Working for some social cause with the help of Data Science for Social Goods Committee, where our team developed a product called \"Let\\'s find a missing Child\" for helping society. company - IBM India pvt ltd description - Mostly worked on blumix and IBM Watson for Data science. '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resumes.loc[df_resumes[\"resume_id\"] == 4, \"resume_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae039c71-dadf-4db2-b049-27b705c0a18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  onetsoc_code                            job_title  \\\n",
      "0   11-1011.00                     Chief Executives   \n",
      "1   11-1011.03        Chief Sustainability Officers   \n",
      "2   11-1021.00      General and Operations Managers   \n",
      "3   11-1031.00                          Legislators   \n",
      "4   11-2011.00  Advertising and Promotions Managers   \n",
      "\n",
      "                                     job_description  \n",
      "0  Determine and formulate policies and provide o...  \n",
      "1  Communicate and coordinate with management, sh...  \n",
      "2  Plan, direct, or coordinate the operations of ...  \n",
      "3  Develop, introduce, or enact laws and statutes...  \n",
      "4  Plan, direct, or coordinate advertising polici...  \n"
     ]
    }
   ],
   "source": [
    "# Load resumes\n",
    "\n",
    "# Load O*NET job descriptions\n",
    "df_onet_jobs = pd.read_csv(\"../data/annotations_scenario_1/processed_onet_occupation.csv\")\n",
    "\n",
    "# Display job descriptions to verify structure\n",
    "print(df_onet_jobs.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58ca9025-b8e9-4d34-82cf-d7b71a4a7041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     category  \\\n",
      "0                Data Science   \n",
      "1             Human Resources   \n",
      "2                    Advocate   \n",
      "3                Web Designer   \n",
      "4         Mechanical Engineer   \n",
      "5                       Sales   \n",
      "6          Health and fitness   \n",
      "7              Civil Engineer   \n",
      "8              Java Developer   \n",
      "9            Business Analyst   \n",
      "10              SAP Developer   \n",
      "11         Automation Testing   \n",
      "12     Electrical Engineering   \n",
      "13         Operations Manager   \n",
      "14           Python Developer   \n",
      "15            DevOps Engineer   \n",
      "16  Network Security Engineer   \n",
      "17                        PMO   \n",
      "18                   Database   \n",
      "19                     Hadoop   \n",
      "20              ETL Developer   \n",
      "21           DotNet Developer   \n",
      "22                 Blockchain   \n",
      "23                    Testing   \n",
      "\n",
      "                                            job_title onetsoc_code  \n",
      "0                                     Data Scientists   15-2051.00  \n",
      "1                         Human Resources Specialists   13-1071.00  \n",
      "2                                            Advocate   21-1093.00  \n",
      "3                                        Web Designer   15-1254.00  \n",
      "4                                 Mechanical Engineer   17-2141.00  \n",
      "5                                      Sales Managers   11-2022.00  \n",
      "6                                      Wellness Coach   11-9179.01  \n",
      "7                                      Civil Engineer   17-2051.00  \n",
      "8                                      Java Developer   15-1251.00  \n",
      "9                                    Business Analyst   13-1111.00  \n",
      "10                                   Software Analyst   15-1211.00  \n",
      "11                                  Automation Tester   15-1253.00  \n",
      "12                               Electrical Engineers   17-2071.00  \n",
      "13                                 Operations Manager   11-1021.00  \n",
      "14                                         Programmer   15-1251.00  \n",
      "15                                    DevOps Engineer   15-1252.00  \n",
      "16                          Network Security Engineer   15-1299.04  \n",
      "17                                  Personnel Officer   13-1071.00  \n",
      "18                                   Database Manager   15-1242.00  \n",
      "19                            Data Storage Specialist   15-1242.00  \n",
      "20  Electronic Data Interchange System Developer (...   15-1299.08  \n",
      "21                                     .NET Developer   15-1252.00  \n",
      "22                               Blockchain Developer   15-1299.07  \n",
      "23                                             Tester   15-1299.04  \n"
     ]
    }
   ],
   "source": [
    "# Define the research-specific job mapping\n",
    "research_jobs = {\n",
    "    \"Data Science\": [\"Data Scientists\", \"15-2051.00\"],\n",
    "    \"Human Resources\": [\"Human Resources Specialists\", \"13-1071.00\"],\n",
    "    \"Advocate\": [\"Advocate\", \"21-1093.00\"],\n",
    "    \"Web Designer\": [\"Web Designer\", \"15-1254.00\"],\n",
    "    \"Mechanical Engineer\": [\"Mechanical Engineer\", \"17-2141.00\"],\n",
    "    \"Sales\": [\"Sales Managers\", \"11-2022.00\"],\n",
    "    \"Health and fitness\": [\"Wellness Coach\", \"11-9179.01\"],\n",
    "    \"Civil Engineer\": [\"Civil Engineer\", \"17-2051.00\"],\n",
    "    \"Java Developer\": [\"Java Developer\", \"15-1251.00\"],\n",
    "    \"Business Analyst\": [\"Business Analyst\", \"13-1111.00\"],\n",
    "    \"SAP Developer\": [\"Software Analyst\", \"15-1211.00\"],\n",
    "    \"Automation Testing\": [\"Automation Tester\", \"15-1253.00\"],\n",
    "    \"Electrical Engineering\": [\"Electrical Engineers\", \"17-2071.00\"],\n",
    "    \"Operations Manager\": [\"Operations Manager\", \"11-1021.00\"],\n",
    "    \"Python Developer\": [\"Programmer\", \"15-1251.00\"],\n",
    "    \"DevOps Engineer\": [\"DevOps Engineer\", \"15-1252.00\"],\n",
    "    \"Network Security Engineer\": [\"Network Security Engineer\", \"15-1299.04\"],\n",
    "    \"PMO\": [\"Personnel Officer\", \"13-1071.00\"],\n",
    "    \"Database\": [\"Database Manager\", \"15-1242.00\"],\n",
    "    \"Hadoop\": [\"Data Storage Specialist\", \"15-1242.00\"],\n",
    "    \"ETL Developer\": [\"Electronic Data Interchange System Developer (EDI System Developer)\", \"15-1299.08\"],\n",
    "    \"DotNet Developer\": [\".NET Developer\", \"15-1252.00\"],\n",
    "    \"Blockchain\": [\"Blockchain Developer\", \"15-1299.07\"],\n",
    "    \"Testing\": [\"Tester\", \"15-1299.04\"]\n",
    "}\n",
    "\n",
    "# Convert to a lookup DataFrame for easier filtering\n",
    "df_research_jobs = pd.DataFrame([\n",
    "    {\"category\": key, \"job_title\": value[0], \"onetsoc_code\": value[1]} for key, value in research_jobs.items()\n",
    "])\n",
    "\n",
    "# Display the research job list for verification\n",
    "print(df_research_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5774a4f8-7883-4d3a-9b11-f78ad230c629",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/annotations_scenario_1/ragatouille_final_predicted_jobs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load predicted jobs from ColBERT matching\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_predicted_jobs \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/annotations_scenario_1/ragatouille_final_predicted_jobs.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Merge predicted jobs with research jobs to filter only relevant ones\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df_filtered_jobs \u001b[38;5;241m=\u001b[39m df_predicted_jobs\u001b[38;5;241m.\u001b[39mmerge(df_research_jobs, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_title\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/annotations_scenario_1/ragatouille_final_predicted_jobs.csv'"
     ]
    }
   ],
   "source": [
    "# Load predicted jobs from ColBERT matching\n",
    "df_predicted_jobs = pd.read_csv(\"./data/annotations_scenario_1/ragatouille_final_predicted_jobs.csv\")\n",
    "\n",
    "# Merge predicted jobs with research jobs to filter only relevant ones\n",
    "df_filtered_jobs = df_predicted_jobs.merge(df_research_jobs, on=\"job_title\", how=\"inner\")\n",
    "\n",
    "# Display filtered results\n",
    "print(\"\\n🎯 Filtered Predicted Jobs (Only Relevant to Research):\")\n",
    "print(df_filtered_jobs.head())\n",
    "\n",
    "# Save filtered job predictions\n",
    "df_filtered_jobs.to_csv(\"../data/annotations_scenario_1/ragatouille_filtered_research_jobs.csv\", index=False)\n",
    "print(\"✅ Filtered research jobs saved as 'ragatouille_filtered_research_jobs.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
