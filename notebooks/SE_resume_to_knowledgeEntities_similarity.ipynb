{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa03d54d-1321-4350-ba36-410cec567dc8",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Resume-to-O*NET Knowledge Mapping (High-Quality Annotations)\n",
    "\n",
    "### Purpose\n",
    "This notebook demonstrates a focused exploration of how high-quality resumes align with the **O*NET Knowledge taxonomy** using semantic similarity techniques.\n",
    "\n",
    "> âœ… This exploration is based on:\n",
    "> - **Objective 1**\n",
    "> - **Annotations Set 1**\n",
    "> - Resumes with a **rating score of 5**\n",
    "\n",
    "The goal is to:\n",
    "- Extract meaningful **noun phrases** from resume text\n",
    "- Match them semantically to relevant **O*NET knowledge entities**\n",
    "- Assess the **semantic similarity** between the resume content and standardized knowledge requirements\n",
    "- Compare against the **importance (`data_value`)** of those knowledge areas for a given job role\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Methodology\n",
    "\n",
    "1. **Data Selection**  \n",
    "   - Resumes are queried from a SQLite database where the annotation rating is 5.\n",
    "   - Each resume is associated with a predicted job title.\n",
    "\n",
    "2. **Text Processing**  \n",
    "   - `TextBlob` is used to extract **noun phrases** from each resume, replicating the method described in the referenced research.\n",
    "\n",
    "3. **Knowledge Entity Matching**  \n",
    "   - For each resume, we select the corresponding **O*NET knowledge entities** by fuzzy-matching the job title to the `job_title` column in the O*NET knowledge dataset.\n",
    "\n",
    "4. **Embedding & Similarity Scoring**  \n",
    "   - Both noun phrases and knowledge entities are encoded using the `all-MiniLM-L6-v2` **SentenceTransformer**.\n",
    "   - **Cosine similarity** is calculated between each noun phrase and each knowledge entity.\n",
    "   - All matches with **similarity â‰¥ 0.65** are retained.\n",
    "\n",
    "5. **Output Construction**  \n",
    "   - For each `(noun phrase, knowledge entity)` pair, the following are recorded:\n",
    "     - Resume ID\n",
    "     - Job Title\n",
    "     - Noun Phrase\n",
    "     - O*NET Knowledge Entity\n",
    "     - Cosine Similarity Score\n",
    "     - O*NET `data_value` (importance level of the knowledge for that job)\n",
    "\n",
    "---\n",
    "\n",
    "### Outcome\n",
    "The output DataFrame provides an interpretable mapping between **resume content and job-specific knowledge requirements**. It enables:\n",
    "- Visualizing **how well a resume covers the most important knowledge areas** for a job\n",
    "- Exploring **semantic overlap** between applicant experience and formal occupation standards\n",
    "- Evaluating entity alignment at a **granular, phrase-level resolution**\n",
    "\n",
    "This exploration supports downstream use cases like:\n",
    "- Automated resume-job fit scoring\n",
    "- Gap analysis between candidate skills and job requirements\n",
    "- Training data analysis for classification models\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "Alonso, R., DessÃ­, D., Meloni, A., & Reforgiato Recupero, D. (2025).  \n",
    "**A novel approach for job matching and skill recommendation using transformers and the O\\*NET database**.  \n",
    "*Big Data Research, 39*, 100509. [DOI: 10.1016/j.bdr.2024.100509](https://doi.org/10.1016/j.bdr.2024.100509)\n",
    "\n",
    "This notebook replicates and adapts core techniques from the above paper, including noun phrase extraction with TextBlob and semantic similarity scoring against O\\*NET entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3dc6522-98d2-436c-8a49-2a985826daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   resume_id                              job_title  \\\n",
      "0          4                   Computer Programmers   \n",
      "1          4                   Computer Programmers   \n",
      "2          4                   Computer Programmers   \n",
      "3          4                   Computer Programmers   \n",
      "4          4  Computer Systems Engineers/Architects   \n",
      "\n",
      "                 noun_phrase           knowledge_entity  similarity_score  \\\n",
      "0                electronics  Computers and Electronics          0.840779   \n",
      "1                electronics  Computers and Electronics          0.840779   \n",
      "2  communication electronics  Computers and Electronics          0.679181   \n",
      "3  communication electronics  Computers and Electronics          0.679181   \n",
      "4                electronics  Computers and Electronics          0.840779   \n",
      "\n",
      "   data_value  \n",
      "0        4.87  \n",
      "1        6.16  \n",
      "2        4.87  \n",
      "3        6.16  \n",
      "4        4.91  \n",
      "âœ… Done computing similarity based on noun phrases.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = \"../data/annotations_scenario_1/annotations_scenario_1.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Step 1: Query 10 resumes with rating = 5\n",
    "query_resumes = \"\"\"\n",
    "SELECT r.id AS resume_id, r.resume_text, pj.job_title\n",
    "FROM resumes r\n",
    "JOIN annotations a ON r.id = a.resume_id\n",
    "JOIN predicted_jobs pj ON r.id = pj.resume_id\n",
    "WHERE a.rating = 5\n",
    "limit 20;\n",
    "\"\"\"\n",
    "df_resumes = pd.read_sql_query(query_resumes, conn)\n",
    "conn.close()\n",
    "\n",
    "# Step 2: Load the O*NET knowledge dataset (previously processed)\n",
    "df_onet = pd.read_csv(\"../data/annotations_scenario_1/processed_onet_knowledge.csv\")\n",
    "\n",
    "# Step 3: Initialize a list to collect similarity results\n",
    "similarity_results = []\n",
    "\n",
    "# Step 4: Iterate over resumes\n",
    "for _, row in df_resumes.iterrows():\n",
    "    resume_id = row[\"resume_id\"]\n",
    "    resume_text = row[\"resume_text\"]\n",
    "    job_title = row[\"job_title\"]\n",
    "\n",
    "    # Get knowledge entities for this job title using pattern matching\n",
    "    df_knowledge = df_onet[df_onet[\"job_title\"].str.contains(job_title, case=False, na=False, regex=True)]\n",
    "\n",
    "    if df_knowledge.empty:\n",
    "        print(f\"âš ï¸ No knowledge entities found for job: {job_title} (Resume ID: {resume_id})\")\n",
    "        continue\n",
    "\n",
    "    # Extract noun phrases from the resume\n",
    "    blob = TextBlob(resume_text)\n",
    "    noun_phrases = list(set(blob.noun_phrases))  # Remove duplicates\n",
    "\n",
    "    if not noun_phrases:\n",
    "        print(f\"âš ï¸ No noun phrases found in resume ID {resume_id}\")\n",
    "        continue\n",
    "\n",
    "    # Encode noun phrases and knowledge entities\n",
    "    resume_embeddings = model.encode(noun_phrases, convert_to_numpy=True)\n",
    "    knowledge_entities = df_knowledge[\"knowledge_entity\"].tolist()\n",
    "    knowledge_embeddings = model.encode(knowledge_entities, convert_to_numpy=True)\n",
    "\n",
    "    # Compute pairwise cosine similarity\n",
    "    similarity_matrix = cosine_similarity(resume_embeddings, knowledge_embeddings)\n",
    "\n",
    "    # Store all similarity scores â‰¥ 0.65, and include data_value from df_knowledge\n",
    "    threshold = 0.65\n",
    "    for i, noun_phrase in enumerate(noun_phrases):\n",
    "        for j, knowledge_entity in enumerate(knowledge_entities):\n",
    "            score = similarity_matrix[i, j]\n",
    "            if score >= threshold:\n",
    "                data_value = df_knowledge.iloc[j][\"data_value\"]\n",
    "                similarity_results.append({\n",
    "                    \"resume_id\": resume_id,\n",
    "                    \"job_title\": job_title,\n",
    "                    \"noun_phrase\": noun_phrase,\n",
    "                    \"knowledge_entity\": knowledge_entity,\n",
    "                    \"similarity_score\": score,\n",
    "                    \"data_value\": data_value\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_similarity = pd.DataFrame(similarity_results).drop_duplicates()\n",
    "\n",
    "# Display the similarity results\n",
    "print(df_similarity.head())\n",
    "\n",
    "# (Optional) Save to CSV\n",
    "# df_similarity.to_csv(\"resume_knowledge_similarity_matrix.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Done computing similarity based on noun phrases.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5ca5837-c5e0-469e-b304-13c35d447d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   resume_id  count\n",
       "0          6     50\n",
       "1          4     20\n",
       "2         11     20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity[\"resume_id\"].value_counts().reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
