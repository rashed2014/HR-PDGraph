{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f807bd-ea1a-4938-a89a-3d4652391cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded O*NET entity files: ['abilities', 'skills', 'work_activities', 'knowledge']\n",
      "\n",
      "üìä Traits with no match: 2 / 120\n",
      "üì¶ Match counts by category:\n",
      "   abilities: 58\n",
      "   skills: 45\n",
      "   knowledge: 32\n",
      "\n",
      "üîÅ Traits that matched multiple categories: 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# ----------------------\n",
    "# Define normalization function\n",
    "# ----------------------\n",
    "def normalize(text):\n",
    "    return text.lower().replace('.', ' ').strip()\n",
    "\n",
    "# ----------------------\n",
    "# Load O*NET entity CSVs\n",
    "# ----------------------\n",
    "onet_dir = \"../data/o_net_files\"\n",
    "entity_dfs = {}\n",
    "\n",
    "for filename in os.listdir(onet_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        category = os.path.splitext(filename)[0].lower()\n",
    "        file_path = os.path.join(onet_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        entity_dfs[category] = df\n",
    "\n",
    "print(\"‚úÖ Loaded O*NET entity files:\", list(entity_dfs.keys()))\n",
    "\n",
    "# ----------------------\n",
    "# Load VOLCANO job_trait file\n",
    "# ----------------------\n",
    "volcano_dir = \"../data/volcano\"\n",
    "job_trait_file = \"Job_traits.csv\"\n",
    "job_traits_df = pd.read_csv(os.path.join(volcano_dir, job_trait_file))\n",
    "\n",
    "# ----------------------\n",
    "# Map each trait to possible KSA matches\n",
    "# ----------------------\n",
    "from collections import defaultdict\n",
    "\n",
    "# Track stats\n",
    "no_match_traits = []\n",
    "match_counts_by_category = defaultdict(int)\n",
    "multi_match_traits = {}\n",
    "\n",
    "for trait in job_traits_df[\"Job_Trait\"].unique():\n",
    "    norm_trait = normalize(trait)\n",
    "    matches = []\n",
    "\n",
    "    for category, df in entity_dfs.items():\n",
    "        ksa_column = [col for col in df.columns if \"_entity\" in col][0]\n",
    "        norm_ksas = df[ksa_column].dropna().apply(normalize).unique()\n",
    "        close_matches = get_close_matches(norm_trait, norm_ksas, n=5, cutoff=0.8)\n",
    "\n",
    "        for match in close_matches:\n",
    "            matches.append((match, category))\n",
    "            match_counts_by_category[category] += 1\n",
    "\n",
    "    if not matches:\n",
    "        no_match_traits.append(trait)\n",
    "    elif len(set([m[1] for m in matches])) > 1:\n",
    "        multi_match_traits[trait] = matches\n",
    "\n",
    "    trait_to_ksas[trait] = matches\n",
    "\n",
    "# Report\n",
    "print(f\"\\nüìä Traits with no match: {len(no_match_traits)} / {len(job_traits_df)}\")\n",
    "print(f\"üì¶ Match counts by category:\")\n",
    "for k, v in match_counts_by_category.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "print(f\"\\nüîÅ Traits that matched multiple categories: {len(multi_match_traits)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbee1982-2ce6-4419-9f2d-43d0e56f03bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved trait-to-KSA matches to CSV.\n"
     ]
    }
   ],
   "source": [
    "trait_rows = []\n",
    "for trait, matches in trait_to_ksas.items():\n",
    "    for match_text, category in matches:\n",
    "        trait_rows.append({\n",
    "            \"trait_text\": trait,\n",
    "            \"matched_entity_text\": match_text,\n",
    "            \"matched_category\": category\n",
    "        })\n",
    "\n",
    "df_trait_links = pd.DataFrame(trait_rows)\n",
    "df_trait_links.to_csv(\"../data/volcano/trait_to_ksas_mapping.csv\", index=False)\n",
    "print(\"‚úÖ Saved trait-to-KSA matches to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbf54bc2-8bbd-4f5e-8cf9-08f3c79df43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mapped 317 occupations to job titles. Saved to:\n",
      "../data/volcano/occupation_to_jobtitle_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# ----------- Load Files -----------\n",
    "volcano_path = \"../data/volcano/Occupations.csv\"\n",
    "onet_dir = \"../data/o_net_files\"\n",
    "\n",
    "# Load the volcano occupations file\n",
    "volcano_df = pd.read_csv(volcano_path)\n",
    "\n",
    "# Load O*NET job titles from all entity CSVs\n",
    "onet_jobtitle_rows = []\n",
    "for file in os.listdir(onet_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(onet_dir, file))\n",
    "        if \"onetsoc_code\" in df.columns and \"job_title\" in df.columns:\n",
    "            onet_jobtitle_rows.append(df[[\"onetsoc_code\", \"job_title\"]])\n",
    "\n",
    "jobtitles_df = pd.concat(onet_jobtitle_rows).drop_duplicates()\n",
    "\n",
    "# ----------- Matching Logic -----------\n",
    "\n",
    "matches = []\n",
    "\n",
    "for _, row in volcano_df.iterrows():\n",
    "    occ_name = row[\"Occupation\"]\n",
    "    \n",
    "    norm_occ = normalize(occ_name)\n",
    "    norm_jobs = jobtitles_df[\"job_title\"].dropna().apply(normalize).unique()\n",
    "    match = get_close_matches(norm_occ, norm_jobs, n=1, cutoff=0.85)\n",
    "    \n",
    "    if match:\n",
    "        matched_title = match[0]\n",
    "        matched_row = jobtitles_df[jobtitles_df[\"job_title\"].apply(normalize) == matched_title]\n",
    "        if not matched_row.empty:\n",
    "            onetsoc_code = matched_row[\"onetsoc_code\"].values[0]\n",
    "            matches.append({\n",
    "                \"occupation_name\": occ_name,\n",
    "                \"matched_job_title\": matched_title,\n",
    "                \"onetsoc_code\": onetsoc_code\n",
    "            })\n",
    "\n",
    "# Save output\n",
    "mapped_df = pd.DataFrame(matches)\n",
    "output_path = \"../data/volcano/occupation_to_jobtitle_mapping.csv\"\n",
    "mapped_df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Mapped {len(mapped_df)} occupations to job titles. Saved to:\\n{output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6005de-eb72-4b02-b23c-72be3c2a8d6c",
   "metadata": {},
   "source": [
    "### Option: Filter the job zones to job titles from researh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d06f0c9-5eb3-4395-8219-38cf0a6d48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# # -----------------------------\n",
    "# # Paths\n",
    "# # -----------------------------\n",
    "# volcano_dir = \"../data/volcano\"\n",
    "# onet_dir = \"../data/o_net_files\"\n",
    "# mapping_path = os.path.join(volcano_dir, \"occupation_to_jobtitle_mapping.csv\")\n",
    "# output_path = os.path.join(onet_dir, \"filtered_occupation_mapping.csv\")\n",
    "# jobtitle_json_path = os.path.join(onet_dir, \"jobs_titles.json\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # Step 1: Load JSON and extract SOC codes\n",
    "# # -----------------------------\n",
    "# with open(jobtitle_json_path, \"r\") as f:\n",
    "#     jobtitle_map = json.load(f)\n",
    "\n",
    "# focus_soc_codes = set([v[1] for v in jobtitle_map.values()])\n",
    "# print(f\"‚úÖ Found {len(focus_soc_codes)} SOC codes in job_titles.json\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # Step 2: Load full occupation mapping\n",
    "# # -----------------------------\n",
    "# df_full = pd.read_csv(mapping_path)\n",
    "\n",
    "# # -----------------------------\n",
    "# # Step 3: Filter by SOC codes\n",
    "# # -----------------------------\n",
    "# df_filtered = df_full[df_full[\"onetsoc_code\"].isin(focus_soc_codes)]\n",
    "\n",
    "# # -----------------------------\n",
    "# # Step 4: Save filtered version\n",
    "# # -----------------------------\n",
    "# df_filtered.to_csv(output_path, index=False)\n",
    "# print(f\"‚úÖ Saved filtered mapping with {len(df_filtered)} rows to:\\n{output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
