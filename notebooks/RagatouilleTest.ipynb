{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bdfc9b5-4ec5-4076-8831-51f25f35e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME is set to: /usr/local/cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "os.environ[\"PATH\"] = f\"{os.environ['CUDA_HOME']}/bin:\" + os.environ[\"PATH\"] \n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{os.environ['CUDA_HOME']}/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\") \n",
    "# Verify the CUDA_HOME variable\n",
    "print(\"CUDA_HOME is set to:\", os.environ[\"CUDA_HOME\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2326019a-16f1-41b0-b22f-e97b9517b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6124ae-f31d-4474-8ab1-44808b63e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16e9f4a-c900-46e1-9a51-e318cf823e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891e54a-16e8-4565-b3b0-50a70aa2ccb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Apr 03, 14:40:09] #> Note: Output directory .ragatouille/colbert/indexes/Miyazaki-123 already exists\n",
      "\n",
      "\n",
      "[Apr 03, 14:40:09] #> Will delete 1 files already at .ragatouille/colbert/indexes/Miyazaki-123 in 20 seconds...\n",
      "#> Starting...\n",
      "#> Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nranks = 2 \t num_gpus = 2 \t device=1\n",
      "[Apr 03, 14:40:34] [1] \t\t #> Encoding 61 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "/opt/conda/lib/python3.11/site-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nranks = 2 \t num_gpus = 2 \t device=0\n",
      "[Apr 03, 14:40:34] [0] \t\t #> Encoding 62 passages..\n",
      "[Apr 03, 14:40:35] [1] \t\t avg_doclen_est = 132.53253173828125 \t len(local_sample) = 61\n",
      "[Apr 03, 14:40:35] [0] \t\t avg_doclen_est = 132.53253173828125 \t len(local_sample) = 62\n",
      "[Apr 03, 14:40:35] [0] \t\t Creating 1,024 partitions.\n",
      "[Apr 03, 14:40:35] [0] \t\t *Estimated* 16,301 embeddings.\n",
      "[Apr 03, 14:40:35] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/Miyazaki-123/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sub_sample = torch.load(sub_sample_path)\n",
      "WARNING clustering 15485 points to 1024 centroids: please provide at least 39936 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 15485 points in 128D to 1024 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "[Apr 03, 14:40:36] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2104, in _run_ninja_build\n",
      "    subprocess.run(\n",
      "  File \"/opt/conda/lib/python3.11/subprocess.py\", line 571, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/infra/launcher.py\", line 134, in setup_new_process\n",
      "    return_val = callee(config, *args)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py\", line 33, in encode\n",
      "    encoder.run(shared_lists)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py\", line 68, in run\n",
      "    self.train(shared_lists) # Trains centroids from selected passages\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py\", line 237, in train\n",
      "    bucket_cutoffs, bucket_weights, avg_residual = self._compute_avg_residual(centroids, heldout)\n",
      "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/collection_indexer.py\", line 315, in _compute_avg_residual\n",
      "    compressor = ResidualCodec(config=self.config, centroids=centroids, avg_residual=None)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py\", line 24, in __init__\n",
      "    ResidualCodec.try_load_torch_extensions(self.use_gpu)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/residual.py\", line 103, in try_load_torch_extensions\n",
      "    decompress_residuals_cpp = load(\n",
      "                               ^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1314, in load\n",
      "    return _jit_compile(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1721, in _jit_compile\n",
      "    _write_ninja_file_and_build_library(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 1833, in _write_ninja_file_and_build_library\n",
      "    _run_ninja_build(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 2120, in _run_ninja_build\n",
      "    raise RuntimeError(message) from e\n",
      "RuntimeError: Error building extension 'decompress_residuals_cpp': [1/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output decompress_residuals.cuda.o.d -DTORCH_EXTENSION_NAME=decompress_residuals_cpp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -std=c++17 -c /opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/decompress_residuals.cu -o decompress_residuals.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mdecompress_residuals.cuda.o \n",
      "/usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output decompress_residuals.cuda.o.d -DTORCH_EXTENSION_NAME=decompress_residuals_cpp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -std=c++17 -c /opt/conda/lib/python3.11/site-packages/colbert/indexing/codecs/decompress_residuals.cu -o decompress_residuals.cuda.o \n",
      "/bin/sh: 1: /usr/local/cuda/bin/nvcc: not found\n",
      "ninja: build stopped: subcommand failed.\n",
      "\n",
      "[rank0]:[W403 14:40:36.044714371 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "[rank1]:[E403 14:50:35.607829687 ProcessGroupNCCL.cpp:616] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600090 milliseconds before timing out.\n",
      "[rank1]:[E403 14:50:35.632476331 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 1] Exception (either an error or timeout) detected by watchdog at work: 5, last enqueued NCCL work: 5, last completed NCCL work: 4.\n",
      "[rank1]:[E403 14:50:35.632494655 ProcessGroupNCCL.cpp:1834] [PG ID 0 PG GUID 0(default_pg) Rank 1] Timeout at NCCL work: 5, last enqueued NCCL work: 5, last completed NCCL work: 4.\n",
      "[rank1]:[E403 14:50:35.632505886 ProcessGroupNCCL.cpp:630] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "[rank1]:[E403 14:50:35.632510044 ProcessGroupNCCL.cpp:636] [Rank 1] To avoid data inconsistency, we are taking the entire process down.\n",
      "[rank1]:[E403 14:50:35.642727799 ProcessGroupNCCL.cpp:1595] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600090 milliseconds before timing out.\n",
      "Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x737e5ce78446 in /opt/conda/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x737e11fbea92 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x737e11fc5ed3 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x737e11fc793d in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: <unknown function> + 0x145c0 (0x737e5cfdf5c0 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch.so)\n",
      "frame #5: <unknown function> + 0x9ca94 (0x737e5dce3a94 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #6: __clone + 0x44 (0x737e5dd70a34 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "terminate called after throwing an instance of 'c10::DistBackendError'\n",
      "  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600090 milliseconds before timing out.\n",
      "Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x737e5ce78446 in /opt/conda/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x737e11fbea92 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x737e11fc5ed3 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x737e11fc793d in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: <unknown function> + 0x145c0 (0x737e5cfdf5c0 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch.so)\n",
      "frame #5: <unknown function> + 0x9ca94 (0x737e5dce3a94 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #6: __clone + 0x44 (0x737e5dd70a34 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x737e5ce78446 in /opt/conda/lib/python3.11/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0xe7eb1b (0x737e11c3cb1b in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: <unknown function> + 0x145c0 (0x737e5cfdf5c0 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch.so)\n",
      "frame #3: <unknown function> + 0x9ca94 (0x737e5dce3a94 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #4: __clone + 0x44 (0x737e5dd70a34 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    "    use_faiss=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fada8-c307-4054-ba07-9486e4fc8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce8d9c-5534-40da-8576-c7d8dc3fb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384212d-6ae0-44ec-9df1-a335253fca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "# Load ColBERT-based RAG model from Ragatouille\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560bf89d-41ef-4926-9dc8-cc11382314f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Group by resume_id to ensure one resume per ID\n",
    "df_resumes_grouped = df_resumes.groupby(\"resume_id\")[\"resume_text\"].first().reset_index()\n",
    "\n",
    "# Step 2: Index Each Resume Independently\n",
    "for resume_id, resume_text in zip(df_resumes_grouped[\"resume_id\"], df_resumes_grouped[\"resume_text\"]):\n",
    "    index_name = f\"resume_{resume_id}\"  # Unique index name per resume\n",
    "    \n",
    "    print(f\"Indexing Resume ID: {resume_id}...\")  # Debugging output\n",
    "    RAG.index(\n",
    "        collection=[resume_text],  # Store the full resume as a single document\n",
    "        index_name=index_name,\n",
    "        max_document_length=180,\n",
    "        split_documents=True,  # Ragatouille will handle chunking\n",
    "        use_faiss=True\n",
    "    )\n",
    "\n",
    "print(\"âœ… All resumes have been indexed successfully (only once per ID)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcbdce-3066-4937-80d6-fc9b774a378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Step 1: Prepare queries (all unique O*NET knowledge entities)\n",
    "knowledge_queries = df_onet[\"knowledge_entity\"].drop_duplicates().tolist()\n",
    "\n",
    "# Step 2: Search for each indexed resume\n",
    "similarity_results = []\n",
    "for resume_id in df_resumes[\"resume_id\"].unique():\n",
    "    index_name = f\"resume_{resume_id}\"  # Resume index name\n",
    "    print(f\"ðŸ” Searching Resume ID: {resume_id}...\")  # Debugging output\n",
    "\n",
    "    # Iterate over each knowledge query instead of stacking them\n",
    "    for query in knowledge_queries:\n",
    "        retrieved_docs = RAG.search(query=query, index_name=index_name, k=3)  \n",
    "\n",
    "        # Store results with raw similarity scores\n",
    "        for doc in retrieved_docs:\n",
    "            matched_text = doc[\"content\"]\n",
    "            similarity_score = doc[\"score\"]  # Keep raw score\n",
    "\n",
    "            similarity_results.append({\n",
    "                \"resume_id\": resume_id,\n",
    "                \"query\": query,\n",
    "                \"matched_resume_chunk\": matched_text,\n",
    "                \"similarity_score\": round(similarity_score, 4)  # Keep raw score\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_similarity = pd.DataFrame(similarity_results)\n",
    "\n",
    "# Step 3: Find the highest similarity score (raw, unnormalized)\n",
    "if not df_similarity.empty:\n",
    "    max_similarity = df_similarity[\"similarity_score\"].max()\n",
    "    highest_match = df_similarity[df_similarity[\"similarity_score\"] == max_similarity]\n",
    "\n",
    "    print(\"\\nðŸŽ¯ Highest Raw Similarity Score using Ragatouille:\")\n",
    "    print(highest_match)\n",
    "else:\n",
    "    print(\"\\nðŸš¨ No results found.\")\n",
    "\n",
    "# Save to CSV\n",
    "df_similarity.to_csv(\"../data/annotations_scenario_1/ragatouille_resume_knowledge_similarity_matrix.csv\", index=False)\n",
    "print(\"âœ… Similarity matrix saved as 'ragatouille_resume_knowledge_similarity_matrix.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15ec56-7480-4de2-98d3-0ed3a6c96433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter similarity scores >= 10\n",
    "df_filtered = df_similarity[df_similarity[\"similarity_score\"] >= 10]\n",
    "\n",
    "# Display the filtered results\n",
    "print(\"\\nðŸŽ¯ Filtered Results (Scores >= 10):\")\n",
    "print(df_filtered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
